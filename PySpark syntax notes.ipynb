{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Learning Spark__\n",
    "Read: chapters 1-3, 9\n",
    "Do: 4,5,6,7,8. \n",
    "\n",
    "__Introduction to Apache Spark__ videos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create data munging cheat sheet__\n",
    "\n",
    "Columns for filter/subset, create column, create row, etc.\n",
    "\n",
    "Then syntax for R, pandas, Spark, etc\n",
    "\n",
    "R dplyr\n",
    "\n",
    "Spark dataframes: filter, select\n",
    "\n",
    "Python Pandas\n",
    "\n",
    "\n",
    "Columms for: import text file delimeter\n",
    "select, filter, mutate, custom function\n",
    "aggregation exmples\n",
    "join exampels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about Spark, Hadoop, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://spark.apache.org/images/spark-stack.png)\n",
    "\n",
    "__Spark Core__\n",
    "Spark Core contains the basic functionality of Spark, including components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. Spark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Spark’s main programming abstraction. RDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections.\n",
    "\n",
    "__Spark SQL__\n",
    "Spark SQL is Spark’s package for working with structured data. It allows querying data via SQL as well as the Apache Hive variant of SQL—called the Hive Query Language (HQL)—and it supports many sources of data, including Hive tables, Parquet, and JSON. Beyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java, and Scala, all within a single application, thus combining SQL with complex analytics. This tight integration with the rich computing environment provided by Spark makes Spark SQL unlike any other open source data warehouse tool. \n",
    "\n",
    "\n",
    "__Spark optimisations__\n",
    "\n",
    "Vs. batch:\n",
    "* Ceasing actions as soon as they are completed: eg first() stops once it has got first line\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notes for brown bag__\n",
    "\n",
    "Spark 1.0 was released in May 2014\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical Spark workflow\n",
    "\n",
    "1. Create some input RDDs from external data.\n",
    "2. Transform them to define new RDDs using transformations like filter().\n",
    "3. Ask Spark to persist() any intermediate RDDs that will need to be reused.\n",
    "4. Launch actions such as count() and first() to kick off a parallel computation, which is then optimized and executed by Spark.\n",
    "\n",
    "Remember: Spark RDDs don't exist. They are pointers to an original data source (eg. textfile) along with a pipeline of transformations. When an _action_ is called, then the whole pipeline is run to return the requested result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexts & config\n",
    "\n",
    "At a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. --> Accessed via SparkContext. We use the sparkContext to build RDDs.\n",
    "\n",
    "Much of what Spark does is pass functions to worker nodes\n",
    "\n",
    "#### SQLContexts vs. HiveContext - ONLY necessary to access SchemaRDD/Dataframe functionality\n",
    "\n",
    "HiveContext is a superset of the SQLContext. Adding Hive functionality allows you to access Hive tables, UDFs (user-defined functions), SerDes (serialization and deserialization formats), and the Hive query language (HiveQL). \n",
    "\n",
    "So we have two entry points, depending on what functionality we want.\n",
    "\n",
    "You can see below that when you run spark-shell, which is your interactive driver application, it automatically creates a SparkContext defined as `sc` and a HiveContext defined as `sqlContext`. The HiveContext allows you to execute SQL queries _as well as_ Hive commands. (also allow access to Hive Metadata Store?)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x1039b3fd0>\n",
      "<pyspark.sql.context.HiveContext object at 0x104fc3e50>\n"
     ]
    }
   ],
   "source": [
    "print sc\n",
    "print sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.HiveContext object at 0x10508f790>\n"
     ]
    }
   ],
   "source": [
    "## We could also define with\n",
    "# Import Spark SQL\n",
    "from pyspark.sql import HiveContext, Row\n",
    "# Or if you can't include the hive requirements\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "hiveCtx = HiveContext(sc)\n",
    "print hiveCtx \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /Users/Cam/anaconda/lib/python2.7/site-packages/IPython/utils/py3compat.py:286 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2d4edef0d247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"My App\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#setMaster sets address of cluster. AppName so you can find in GUI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Cam/Downloads/spark-1.5.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/Cam/Downloads/spark-1.5.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    248\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 250\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /Users/Cam/anaconda/lib/python2.7/site-packages/IPython/utils/py3compat.py:286 "
     ]
    }
   ],
   "source": [
    "## Initialising sparkcontext with parameters\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\") #setMaster sets address of cluster. AppName so you can find in GUI\n",
    "sc = SparkContext(conf = conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic RDDs\n",
    "\n",
    "In Spark, we express our computation through operations on distributed collections that are automatically parallelized across the cluster. These collections are called resilient distributed datasets, or RDDs. RDDs are Spark’s fundamental abstraction for distributed data and computation.\n",
    "\n",
    "In Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. Under the hood, Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating RDDs__\n",
    "\n",
    "The simplest way to create RDDs is to take an existing collection in your program and pass it to SparkContext’s parallelize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[10] at parallelize at PythonRDD.scala:391"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =[1,2,3,4,5]\n",
    "rDD= sc.parallelize(data,4)\n",
    "rDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From textfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "# Apache Spark\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"README.md\") # Create an RDD called lines\n",
    "print lines.count() # Count the number of items in this RDD\n",
    "print lines.first() # First item in this RDD, i.e. first line of README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RDD operations: transformations__\n",
    "\n",
    "lazily evaluated\n",
    "\n",
    "\n",
    "__filter()__\t return a new dataset formed by selecting those elements of the source on which func returns true\n",
    "\n",
    "__map()__\t return a new distributed dataset formed by passing each element of the source through a function func. We can use map() to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers. \n",
    "\n",
    "__flatMap()__\tsimilar to map, but each input item can be mapped to 0 or more output items (so func should return a list rather than a single item). Note that the list is FLATTENED before being returned, so all the lists of words will be combined into a single list\n",
    "\n",
    "\n",
    "__distinct()__\t return a new dataset that contains the distinct elements of the source dataset\n",
    "\n",
    "__sample()__   sample with or without replacement\n",
    "\n",
    "\n",
    "\n",
    "_Transformations that apply to multiple RDDs_\n",
    "\n",
    "__union()__\n",
    "\n",
    "__subtract()__ remove contents of one from another - eg remove training sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter()\n",
    "pythonLines = lines.filter(lambda line: \"Python\" in line)\n",
    "\n",
    "# Union = like SQL union for RDDs\n",
    "badLinesRDD = errorsRDD.union(warningsRDD)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "## map() - squaring everything\n",
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x: x * x).collect()\n",
    "for num in squared:\n",
    "    print \"%i \" % (num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more map examples\n",
    "\n",
    "print pandaFriends.map(lambda row: row.name).collect()  # print list of row names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatmap()\n",
    "lines = sc.parallelize([\"hello world\", \"hi\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "words.first()  # returns \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'world'], ['object', 'face'], ['never', 'run', 'away']]\n",
      "['hello', 'world', 'object', 'face', 'never', 'run', 'away']\n"
     ]
    }
   ],
   "source": [
    "# map vs. flatmap\n",
    "lines = sc.parallelize([\"hello world\", \"object face\", \"never run away\"])\n",
    "words = lines.map(lambda line: line.split(\" \"))\n",
    "print words.collect()\n",
    "\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "print words.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RDD Operations: Actions__\n",
    "\n",
    "\n",
    "__reduce()__ aggregate RDD given a function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-00970181501f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpythonLines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count' is not defined"
     ]
    }
   ],
   "source": [
    "## SELECTION\n",
    "collect()  # return ALL ELEMENTS\n",
    "first()\n",
    "take(n)\n",
    "\n",
    "takeOrdered()\n",
    "\n",
    "## Aggregation\n",
    "count()\n",
    "sum = rdd.reduce(lambda x, y: x + y)\n",
    "aggregate()\n",
    "\n",
    "countByValue()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating functions and passing to Spark__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"README.md\") # Create an RDD called lines\n",
    "\n",
    "def hasPython(line):\n",
    "    return \"Python\" in line\n",
    "\n",
    "pythonLines = lines.filter(hasPython)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Lambda functions\n",
    "word = rdd.filter(lambda s: \"error\" in s)\n",
    "# This is the same as:\n",
    "def containsError(s):\n",
    "    return \"error\" in s\n",
    "word = rdd.filter(containsError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RDD operations: persisting in memory__\n",
    "\n",
    "you will often use persist() to load a subset of your data into memory and query it repeatedly. \n",
    "\n",
    "You can set storage location within persist(): `result.persist(StorageLevel.DISK_ONLY)`\n",
    "\n",
    "MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pythonLines.persist   # cache() works the same \n",
    "pythonLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL = Dataframes (\"SchemaRDDs\") and other schema-enhanced data\n",
    "\n",
    "(IF you have structured or semi-structured data including, for example, out of Hive, then this is much more powerful.(\n",
    "\n",
    "\n",
    "\n",
    "Spark’s interface for working with structured and semistructured data. Structured data is any data that has a schema—that is, a known set of fields for each record. When you have this type of data, Spark SQL makes it both easier and more efficient to load and query. In particular, Spark SQL provides three main capabilities:\n",
    "\n",
    "1. It can load data from a variety of structured sources (e.g., JSON, Hive, and Parquet).\n",
    "2. It lets you query the data using SQL, both inside a Spark program and from external tools that connect to Spark SQL through standard database connectors (JDBC/ODBC), such as business intelligence tools like Tableau.\n",
    "3. When used within a Spark program, Spark SQL provides rich integration between SQL and regular Python/Java/Scala code, including the ability to join RDDs and SQL tables, expose custom functions in SQL, and more. Many jobs are easier to write using this combination.\n",
    "\n",
    "To implement these capabilities, Spark SQL provides a special type of RDD called SchemaRDD. A SchemaRDD is an RDD of Row objects, each representing a record. A SchemaRDD also knows the schema (i.e., data fields) of its rows. While SchemaRDDs look like regular RDDs, internally they store data in a more efficient manner, taking advantage of their schema. In addition, they provide new operations not available on RDDs, such as the ability to run SQL queries. SchemaRDDs can be created from external data sources, from the results of queries, or from regular RDDs.\n",
    "\n",
    "_See above for notes on HiveContext vs. more basic SQLContext_\n",
    "\n",
    "__Dataframes__\n",
    "\n",
    "Dataframe = RDD + metadata + optimisation\n",
    "\n",
    "Both loading data and executing queries return SchemaRDDs. SchemaRDDs are similar to tables in a traditional database. Under the hood, a SchemaRDD is an RDD composed of Row objects with additional schema information of the types in each column. Row objects are just wrappers around arrays of basic types (e.g., integers and strings),\n",
    "\n",
    "SchemaRDDs are also regular RDDs, so you can operate on them using existing RDD transformations like map() and filter(). However, they provide several additional capabilities. Most importantly, you can register any SchemaRDD as a temporary table to query it via _HiveContext.sql_ or _SQLContext.sql_. You do so using the SchemaRDD’s `registerTempTable()`.\n",
    "\n",
    "Refer to list of dtypes avaialble for Dataframes: BIGINT, FLOAT, DOUBLE, STRING, BOOLEAN, TIMESTAMP, .... you can also have stuctures nested within strucutres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dataframes: Basic operations__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show schema\n",
    "adultDataFrame.printSchema()\n",
    "\n",
    "## Selecting\n",
    "# Return the column\n",
    "DF.column_name\n",
    "\n",
    "# get row\n",
    "DF.row[i]\n",
    "\n",
    "DF.row[i].column_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data munging__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select columns\n",
    "adultDataFrame.select(\"workclass\",\"age\",\"education\",\"income\").show()\n",
    "\n",
    "# filter rows\n",
    " adultDataFrame\n",
    "      .select(\"workclass\",\"age\",\"education\",\"occupation\",\"income\")\n",
    "      .filter( adultDataFrame(\"age\") > 30 )\n",
    "      .show()\n",
    "\n",
    "# group by\n",
    "    adultDataFrame\n",
    "      .groupBy(\"income\")\n",
    "      .count()\n",
    "      .show()\n",
    "        \n",
    "# sort\n",
    "    adultDataFrame\n",
    "      .groupBy(\"income\",\"occupation\")\n",
    "      .count()\n",
    "      .sort(\"occupation\")\n",
    "      .show()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Registering temp tables__\n",
    "\n",
    "The purpose is to create a point to facilitate SQL querying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tbl = rows.registerTempTable(\"people\")\n",
    "pandaFriends = hiveCtx.sql(\"SELECT name FROM people WHERE favouriteAnimal = \\\"panda\\\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dataframes: 'import' data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert an RDD to a DF - use toDF() OR use inferSchema()\n",
    "df=RDD.toDF()\n",
    "df=RDD.inferSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refer to hive metastore\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "hiveCtx = HiveContext(sc)\n",
    "rows = hiveCtx.sql(\"SELECT key, value FROM mytable\")\n",
    "keys = rows.map(lambda row: row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load from Parquet\n",
    "rows = hiveCtx.parquetFile(parquetFile)\n",
    "names = rows.map(lambda row: row.name)\n",
    "print \"Everyone\"\n",
    "print names.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Hive metadata store\n",
    "\n",
    "    hiveContext.sql(\"\n",
    "\n",
    "        CREATE EXTERNAL TABLE IF NOT EXISTS adult3\n",
    "           (\n",
    "             idx             INT,\n",
    "             age             INT,\n",
    "             workclass       STRING,\n",
    "             fnlwgt          INT,\n",
    "             education       STRING,\n",
    "             educationnum    INT,\n",
    "             maritalstatus   STRING,\n",
    "             occupation      STRING,\n",
    "             relationship    STRING,\n",
    "             race            STRING,\n",
    "             gender          STRING,\n",
    "             capitalgain     INT,\n",
    "             capitalloss     INT,\n",
    "             nativecountry   STRING,\n",
    "             income          STRING\n",
    "           )\n",
    "           ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "           LOCATION '/data/spark/hive'\n",
    "\n",
    "                   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## \"Registering\" a table so can use HiveSQL on it [registed in Hive Metadata ?]\n",
    "\n",
    "input = hiveCtx.jsonFile(inputFile)\n",
    "\n",
    "# Register the input schema RDD\n",
    "input.registerTempTable(\"tweets\")\n",
    "\n",
    "# Select tweets based on the retweetCount\n",
    "topTweets = hiveCtx.sql(\"\"\"SELECT text, retweetCount  FROM\n",
    "  tweets ORDER BY retweetCount LIMIT 10\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various methods of \"import\" = point to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
